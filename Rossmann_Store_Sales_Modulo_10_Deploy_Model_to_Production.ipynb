{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ricardojnf33/Rossmann_Store_Sales/blob/main/Rossmann_Store_Sales_Modulo_10_Deploy_Model_to_Production.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cme0l2fgyTFh"
      },
      "source": [
        "# 0.0. IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htMUGdWbyTFd",
        "outputId": "e6880707-96d6-4085-daea-9cd5c6959731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting inflection\n",
            "  Downloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (5.5.0)\n",
            "Collecting boruta\n",
            "  Downloading Boruta-0.3-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.21.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.2.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython) (5.1.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython) (0.2.5)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from boruta) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.17.1->boruta) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.17.1->boruta) (3.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython) (0.7.0)\n",
            "Installing collected packages: inflection, boruta\n",
            "Successfully installed boruta-0.3 inflection-0.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install inflection seaborn matplotlib IPython boruta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mVnPHVB0yTFj"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import random\n",
        "import pickle\n",
        "import warnings\n",
        "import inflection\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "\n",
        "from matplotlib            import pyplot as plt\n",
        "from IPython.core.display  import HTML\n",
        "from IPython.display       import Image\n",
        "from scipy                 import stats  as ss\n",
        "from boruta                import BorutaPy\n",
        "from random                import sample\n",
        "from flask import Flask, request, Response\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.ensemble      import RandomForestRegressor\n",
        "from sklearn.metrics       import mean_absolute_error, mean_squared_error\n",
        "from sklearn.linear_model  import LinearRegression, Lasso\n",
        "\n",
        "warnings.filterwarnings( 'ignore' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q6NwTF7yTFj"
      },
      "source": [
        "## 0.1. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hvgMZOoeyTFk"
      },
      "outputs": [],
      "source": [
        "def cross_validation( x_training, kfold, model_name, model, verbose=False ):\n",
        "    mae_list = []\n",
        "    mape_list = []\n",
        "    rmse_list = []\n",
        "    for k in reversed( range( 1, kfold+1 ) ):\n",
        "        if verbose:\n",
        "            print( '\\nKFold Number: {}'.format( k ) )\n",
        "        # \"start and end\" date for validation\n",
        "        validation_start_date = x_training['date'].max() - datetime.timedelta( days=k*6*7 )\n",
        "        validation_end_date = x_training['date'].max() - datetime.timedelta( days=(k-1)*6*7 )\n",
        "\n",
        "        # filtering dataset\n",
        "        training = x_training[x_training['date'] < validation_start_date]\n",
        "        validation = x_training[(x_training['date'] >= validation_start_date) & (x_training['date'] <= validation_end_date ) ]\n",
        "\n",
        "        # training and validation dataset\n",
        "        # training\n",
        "        xtraining = training.drop( ['date', 'sales'], axis=1 )\n",
        "        ytraining = training['sales']\n",
        "\n",
        "        # validation\n",
        "        xvalidation = validation.drop( ['date', 'sales'], axis=1 )\n",
        "        yvalidation = validation['sales']\n",
        "\n",
        "        # model\n",
        "        m = model.fit( xtraining, ytraining )\n",
        "\n",
        "        # prediction\n",
        "        yhat = m.predict( xvalidation )\n",
        "\n",
        "        # performance\n",
        "        m_result = ml_error( model_name, np.expm1( yvalidation ), np.expm1( yhat ) )\n",
        "\n",
        "        # store performance of each kfold iteration\n",
        "        mae_list.append( m_result[ 'MAE' ] )\n",
        "        mape_list.append( m_result[ 'MAPE' ] )\n",
        "        rmse_list.append( m_result[ 'RMSE' ] )\n",
        "\n",
        "    return pd.DataFrame( { 'MAE CV': np.round( np.mean( mae_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( mae_list ), 2 ).astype ( str ),\n",
        "                        'MAPE CV': np.round( np.mean( mape_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( mape_list ), 2 ).astype ( str ),\n",
        "                        'RMSE CV': np.round( np.mean( rmse_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( rmse_list ), 2 ).astype ( str ) }, index=[0] )\n",
        "\n",
        "def mean_absolute_percentage_error( y, yhat ):\n",
        "    return np.mean( np.abs( ( y - yhat ) / y)  )\n",
        "\n",
        "\n",
        "def ml_error( model_name, y, yhat ):\n",
        "    mae = mean_absolute_error( y, yhat )\n",
        "    mape = mean_absolute_percentage_error( y, yhat )\n",
        "    rmse = np.sqrt( mean_squared_error( y, yhat ) )\n",
        "\n",
        "    return pd.DataFrame ( { 'Model Name': model_name,\n",
        "                            'MAE': mae,\n",
        "                            'MAPE': mape,\n",
        "                            'RMSE': rmse }, index=[0] ) \n",
        "\n",
        "def cramer_v( x, y ) :\n",
        "    cm = pd.crosstab( x, y ).values    \n",
        "    n = cm.sum()\n",
        "    r, k = cm.shape\n",
        "\n",
        "    chi2 = ss.chi2_contingency( cm )[0]\n",
        "    \n",
        "    chi2corr = max( 0, chi2 - (k-1)*(r-1)/(n-1) ) #Correção do Bias\n",
        "    kcorr = k - (k-1)**2/(n-1)\n",
        "    rcorr = r - (r-1)**2/(n-1)\n",
        "\n",
        "    return np.sqrt( (chi2corr/n) / ( min( kcorr-1,rcorr-1 ) ) )\n",
        "\n",
        "def jupyter_settings():\n",
        "    %matplotlib inline\n",
        "    %pylab inline\n",
        "    \n",
        "    plt.style.use( 'bmh' )\n",
        "    plt.rcParams['figure.figsize'] = [25, 12]\n",
        "    plt.rcParams['font.size'] = 24\n",
        "    \n",
        "    display( HTML( '<style>.container { width:100% !important; }</style>') )\n",
        "    pd.options.display.max_columns = None\n",
        "    pd.options.display.max_rows = None\n",
        "    pd.set_option( 'display.expand_frame_repr', False )\n",
        "    \n",
        "    sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "O8h8HoeqyTFl",
        "outputId": "bb33937f-489b-4d4f-cd38-190f0273ed63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "jupyter_settings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY-VPfJC7sls",
        "outputId": "f6f66ef0-b3ce-47ef-ca2d-8b86bf79bcfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Rossmann_Store_Sales'...\n",
            "remote: Enumerating objects: 107, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 107 (delta 56), reused 32 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (107/107), 17.71 MiB | 8.71 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Ricardojnf33/Rossmann_Store_Sales.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evMVehWtyTFl"
      },
      "source": [
        "## 0.2. Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KOS2CyzJyTFm"
      },
      "outputs": [],
      "source": [
        "df_sales_raw = pd.read_csv( '/content/Rossmann_Store_Sales/Dataset/train.csv', low_memory=False )\n",
        "df_store_raw = pd.read_csv( '/content/Rossmann_Store_Sales/Dataset/test.csv', low_memory=False )\n",
        "\n",
        "# merge\n",
        "df_raw = pd.merge( df_sales_raw, df_store_raw, how='left', on='Store' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAFJ7tGZyTFn"
      },
      "source": [
        "# 1.0. PASSO 01 - DESCRICAO DOS DADOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQW4OupGyTFn"
      },
      "outputs": [],
      "source": [
        "df1 = df_raw.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEPkhdtxyTFo"
      },
      "source": [
        "## 1.1. Rename Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwozLdtfyTFo"
      },
      "outputs": [],
      "source": [
        "cols_old = ['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', \n",
        "            'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
        "            'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
        "\n",
        "snakecase = lambda x: inflection.underscore( x )\n",
        "\n",
        "cols_new = list( map( snakecase, cols_old ) )\n",
        "\n",
        "# rename\n",
        "df1.columns = cols_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU_NLcbfyTFo"
      },
      "source": [
        "## 1.2. Data Dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg1Qv8E0yTFp"
      },
      "outputs": [],
      "source": [
        "print( 'Number of Rows: {}'.format( df1.shape[0] ) )\n",
        "print( 'Number of Cols: {}'.format( df1.shape[1] ) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFO6TERpyTFp"
      },
      "source": [
        "## 1.3. Data Types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92DkoKZMyTFp"
      },
      "outputs": [],
      "source": [
        "df1['date'] = pd.to_datetime( df1['date'] )\n",
        "df1.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQVDL2bzyTFp"
      },
      "source": [
        "## 1.4. Check NA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImK_JqfzyTFq"
      },
      "outputs": [],
      "source": [
        "df1.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVc5kcYLyTFq"
      },
      "source": [
        "## 1.5. Fillout NA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WPMXWkFyTFq"
      },
      "outputs": [],
      "source": [
        "df1.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkU2I4_zyTFq"
      },
      "outputs": [],
      "source": [
        "#competition_distance        \n",
        "df1['competition_distance'] = df1['competition_distance'].apply( lambda x: 200000.0 if math.isnan( x ) else x )\n",
        "\n",
        "#competition_open_since_month\n",
        "df1['competition_open_since_month'] = df1.apply( lambda x: x['date'].month if math.isnan( x['competition_open_since_month'] ) else x['competition_open_since_month'], axis=1 )\n",
        "\n",
        "#competition_open_since_year \n",
        "df1['competition_open_since_year'] = df1.apply( lambda x: x['date'].year if math.isnan( x['competition_open_since_year'] ) else x['competition_open_since_year'], axis=1 )\n",
        "\n",
        "#promo2_since_week           \n",
        "df1['promo2_since_week'] = df1.apply( lambda x: x['date'].week if math.isnan( x['promo2_since_week'] ) else x['promo2_since_week'], axis=1 )\n",
        "\n",
        "#promo2_since_year           \n",
        "df1['promo2_since_year'] = df1.apply( lambda x: x['date'].year if math.isnan( x['promo2_since_year'] ) else x['promo2_since_year'], axis=1 )\n",
        "\n",
        "#promo_interval              \n",
        "month_map = {1: 'Jan',  2: 'Fev',  3: 'Mar',  4: 'Apr',  5: 'May',  6: 'Jun',  7: 'Jul',  8: 'Aug',  9: 'Sep',  10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
        "\n",
        "df1['promo_interval'].fillna(0, inplace=True )\n",
        "\n",
        "df1['month_map'] = df1['date'].dt.month.map( month_map )\n",
        "\n",
        "df1['is_promo'] = df1[['promo_interval', 'month_map']].apply( lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split( ',' ) else 0, axis=1 )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bh5p1YHyTFr"
      },
      "outputs": [],
      "source": [
        "df1.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2jyu4OryTFr"
      },
      "source": [
        "## 1.6. Change Data Types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glXRA3LFyTFr"
      },
      "outputs": [],
      "source": [
        "# competiton\n",
        "df1['competition_open_since_month'] = df1['competition_open_since_month'].astype( int )\n",
        "df1['competition_open_since_year'] = df1['competition_open_since_year'].astype( int )\n",
        "    \n",
        "# promo2\n",
        "df1['promo2_since_week'] = df1['promo2_since_week'].astype( int )\n",
        "df1['promo2_since_year'] = df1['promo2_since_year'].astype( int )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88-Rbd55yTFr"
      },
      "source": [
        "## 1.7. Descriptive Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiN_isnYyTFs"
      },
      "outputs": [],
      "source": [
        "num_attributes = df1.select_dtypes( include=['int64', 'float64'] )\n",
        "cat_attributes = df1.select_dtypes( exclude=['int64', 'float64', 'datetime64[ns]'] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFVOirKVyTFs"
      },
      "source": [
        "### 1.7.1. Numerical Atributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYh7AEClyTFs"
      },
      "outputs": [],
      "source": [
        "# Central Tendency - mean, meadina \n",
        "ct1 = pd.DataFrame( num_attributes.apply( np.mean ) ).T\n",
        "ct2 = pd.DataFrame( num_attributes.apply( np.median ) ).T\n",
        "\n",
        "# dispersion - std, min, max, range, skew, kurtosis\n",
        "d1 = pd.DataFrame( num_attributes.apply( np.std ) ).T \n",
        "d2 = pd.DataFrame( num_attributes.apply( min ) ).T \n",
        "d3 = pd.DataFrame( num_attributes.apply( max ) ).T \n",
        "d4 = pd.DataFrame( num_attributes.apply( lambda x: x.max() - x.min() ) ).T \n",
        "d5 = pd.DataFrame( num_attributes.apply( lambda x: x.skew() ) ).T \n",
        "d6 = pd.DataFrame( num_attributes.apply( lambda x: x.kurtosis() ) ).T \n",
        "\n",
        "# concatenar\n",
        "m = pd.concat( [d2, d3, d4, ct1, ct2, d1, d5, d6] ).T.reset_index()\n",
        "m.columns = ['attributes', 'min', 'max', 'range', 'mean', 'median', 'std', 'skew', 'kurtosis']\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDIPsy2XyTFt"
      },
      "outputs": [],
      "source": [
        "sns.distplot( df1['competition_distance'], kde=False )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JFuaY1VyTFt"
      },
      "source": [
        "### 1.7.2. Categorical Atributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwPcToYdyTFt"
      },
      "outputs": [],
      "source": [
        "cat_attributes.apply( lambda x: x.unique().shape[0] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66_eZK4dyTFt"
      },
      "outputs": [],
      "source": [
        "aux = df1[(df1['state_holiday'] != '0') & (df1['sales'] > 0)]\n",
        "\n",
        "plt.subplot( 1, 3, 1 )\n",
        "sns.boxplot( x='state_holiday', y='sales', data=aux )\n",
        "\n",
        "plt.subplot( 1, 3, 2 )\n",
        "sns.boxplot( x='store_type', y='sales', data=aux )\n",
        "\n",
        "plt.subplot( 1, 3, 3 )\n",
        "sns.boxplot( x='assortment', y='sales', data=aux )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS8hzOB4yTFu"
      },
      "source": [
        "# 2.0. PASSO 02 - FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdA7CVhQyTFu"
      },
      "outputs": [],
      "source": [
        "df2 = df1.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO2hs5lcyTFu"
      },
      "source": [
        "## 2.1. Mapa Mental de Hipoteses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yl27TDD6yTFu"
      },
      "outputs": [],
      "source": [
        "Image( '/content/Rossmann_Store_Sales/MindMapHypothesis.png' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niSe3joDyTFu"
      },
      "source": [
        "## 2.2. Criacao das Hipoteses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CRBfVeJyTFu"
      },
      "source": [
        "### 2.2.1. Hipoteses Loja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNkczX2nyTFv"
      },
      "source": [
        "**1.** Lojas com número maior de funcionários deveriam vender mais.\n",
        "\n",
        "**2.** Lojas com maior capacidade de estoque deveriam vender mais.\n",
        "\n",
        "**3.** Lojas com maior porte deveriam vender mais.\n",
        "\n",
        "**4.** Lojas com maior sortimentos deveriam vender mais.\n",
        "\n",
        "**5.** Lojas com competidores mais próximos deveriam vender menos.\n",
        "\n",
        "**6.** Lojas com competidores à mais tempo deveriam vendem mais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga_QMYZuyTFv"
      },
      "source": [
        "### 2.2.2. Hipoteses Produto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWhvPgFRyTFv"
      },
      "source": [
        "**1.** Lojas que investem mais em Marketing deveriam vender mais.\n",
        "\n",
        "**2.** Lojas com maior exposição de produto deveriam vender mais.\n",
        "\n",
        "**3.** Lojas com produtos com preço menor deveriam vender mais.\n",
        "\n",
        "**5.** Lojas com promoções mais agressivas ( descontos maiores ), deveriam vender mais.\n",
        "\n",
        "**6.** Lojas com promoções ativas por mais tempo deveriam vender mais.\n",
        "\n",
        "**7.** Lojas com mais dias de promoção deveriam vender mais.\n",
        "\n",
        "**8.** Lojas com mais promoções consecutivas deveriam vender mais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWqG1AYJyTFv"
      },
      "source": [
        "### 2.2.3. Hipoteses Tempo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SMbaJRxyTFv"
      },
      "source": [
        "**1.** Lojas abertas durante o feriado de Natal deveriam vender mais.\n",
        "\n",
        "**2.** Lojas deveriam vender mais ao longo dos anos.\n",
        "\n",
        "**3.** Lojas deveriam vender mais no segundo semestre do ano.\n",
        "\n",
        "**4.** Lojas deveriam vender mais depois do dia 10 de cada mês.\n",
        "\n",
        "**5.** Lojas deveriam vender menos aos finais de semana.\n",
        "\n",
        "**6.** Lojas deveriam vender menos durante os feriados escolares."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUeCXTpyyTFv"
      },
      "source": [
        "## 2.3. Lista Final de Hipóteses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr2zDGaXyTFv"
      },
      "source": [
        "**1.** Lojas com maior sortimentos deveriam vender mais.\n",
        "\n",
        "**2.** Lojas com competidores mais próximos deveriam vender menos.\n",
        "\n",
        "**3.** Lojas com competidores à mais tempo deveriam vendem mais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITkuXSN6yTFw"
      },
      "source": [
        "**4.** Lojas com promoções ativas por mais tempo deveriam vender mais.\n",
        "\n",
        "**5.** Lojas com mais dias de promoção deveriam vender mais.\n",
        "\n",
        "**7.** Lojas com mais promoções consecutivas deveriam vender mais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUFM4-o_yTFw"
      },
      "source": [
        "**8.** Lojas abertas durante o feriado de Natal deveriam vender mais.\n",
        "\n",
        "**9.** Lojas deveriam vender mais ao longo dos anos.\n",
        "\n",
        "**10.** Lojas deveriam vender mais no segundo semestre do ano.\n",
        "\n",
        "**11.** Lojas deveriam vender mais depois do dia 10 de cada mês.\n",
        "\n",
        "**12.** Lojas deveriam vender menos aos finais de semana.\n",
        "\n",
        "**13.** Lojas deveriam vender menos durante os feriados escolares.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRiWMOiSyTFw"
      },
      "source": [
        "## 2.4. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBAhO8wiyTFw"
      },
      "outputs": [],
      "source": [
        "# year\n",
        "df2['year'] = df2['date'].dt.year\n",
        "\n",
        "# month\n",
        "df2['month'] = df2['date'].dt.month\n",
        "\n",
        "# day\n",
        "df2['day'] = df2['date'].dt.day\n",
        "\n",
        "# week of year\n",
        "df2['week_of_year'] = df2['date'].dt.isocalendar().week \n",
        "\n",
        "# year week\n",
        "df2['year_week'] = df2['date'].dt.strftime( '%Y-%W' )\n",
        "\n",
        "# competition since\n",
        "df2['competition_since'] = df2.apply( lambda x: datetime.datetime( year=x['competition_open_since_year'], month=x['competition_open_since_month'],day=1 ), axis=1 )\n",
        "df2['competition_time_month'] = ( ( df2['date'] - df2['competition_since'] )/30 ).apply( lambda x: x.days ).astype( int )\n",
        "\n",
        "# promo since\n",
        "df2['promo_since'] = df2['promo2_since_year'].astype( str ) + '-' + df2['promo2_since_week'].astype( str )\n",
        "df2['promo_since'] = df2['promo_since'].apply( lambda x: datetime.datetime.strptime( x + '-1', '%Y-%W-%w' ) - datetime.timedelta( days=7 ) )\n",
        "df2['promo_time_week'] = ( ( df2['date'] - df2['promo_since'] )/7 ).apply( lambda x: x.days ).astype( int )\n",
        "\n",
        "# assortment\n",
        "df2['assortment'] = df2['assortment'].apply( lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended' )\n",
        "\n",
        "# state holiday\n",
        "df2['state_holiday'] = df2['state_holiday'].apply( lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DtHBNRayTFw"
      },
      "source": [
        "# 3.0. PASSO 03 - FILTRAGEM DE VARIÁVEIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1YRpxvOyTFw"
      },
      "outputs": [],
      "source": [
        "df3 = df2.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLq21TCyTFw"
      },
      "source": [
        "## 3.1. Filtragem das Linhas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIywU82fyTFw"
      },
      "outputs": [],
      "source": [
        "df3 = df3[(df3['open'] != 0) & (df3['sales'] > 0)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Mfk5KzKyTFx"
      },
      "source": [
        "## 3.2. Selecao das Colunas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rudz-7njyTFx"
      },
      "outputs": [],
      "source": [
        "cols_drop = ['customers', 'open', 'promo_interval', 'month_map']\n",
        "df3 = df3.drop( cols_drop, axis=1 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU19P0jvyTFx"
      },
      "outputs": [],
      "source": [
        "df3.head().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVgxFl_oml9b"
      },
      "source": [
        "# 4.0. PASSO 04 - ANALISE EXPLORATORIA DE DADOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sl5BtNwmyTFx"
      },
      "outputs": [],
      "source": [
        "df4 = df3.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZFlchIns1FE"
      },
      "source": [
        "## 4.1. ANALISE UNIVARIADA ( Olhar únicamente para cada variável )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4XkvK0TtN6y"
      },
      "source": [
        "### 4.1.1. Response Variable "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1pArXNstqOK"
      },
      "outputs": [],
      "source": [
        "sns.displot( df4['sales'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v-ofk6DtXwk"
      },
      "source": [
        "### 4.1.2. Numerical Variable "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X1rB_DJuDtQ"
      },
      "outputs": [],
      "source": [
        "num_attributes.hist( bins=25 );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMdKWQI_tgFv"
      },
      "source": [
        "### 4.1.3. Categorical Variable "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8anT5Qcu7i7"
      },
      "outputs": [],
      "source": [
        "df4['store_type'].drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wuUpcS8v67M"
      },
      "outputs": [],
      "source": [
        "# state_holiday\n",
        "plt.subplot( 3, 2, 1 )\n",
        "a = df4[df4['state_holiday'] != 'regular_day']\n",
        "sns.countplot( a['state_holiday'] )\n",
        "\n",
        "plt.subplot( 3, 2, 2 )\n",
        "sns.kdeplot( df4[df4['state_holiday'] == 'public_holiday']['sales'], label='public_holiday', shade=True )\n",
        "sns.kdeplot( df4[df4['state_holiday'] == 'easter_holiday']['sales'], label='easter_holiday', shade=True )\n",
        "sns.kdeplot( df4[df4['state_holiday'] == 'christmas']['sales'], label='christmas', shade=True )\n",
        "\n",
        "#store_type\n",
        "plt.subplot( 3, 2, 3 )\n",
        "sns.countplot( df4['store_type'] )\n",
        "\n",
        "plt.subplot( 3, 2, 4 )\n",
        "sns.kdeplot( df4[df4['store_type'] == 'a']['sales'], label='a', shade=True )\n",
        "sns.kdeplot( df4[df4['store_type'] == 'b']['sales'], label='b', shade=True )\n",
        "sns.kdeplot( df4[df4['store_type'] == 'c']['sales'], label='c', shade=True )\n",
        "sns.kdeplot( df4[df4['store_type'] == 'd']['sales'], label='d', shade=True )\n",
        "\n",
        "#assortment\n",
        "plt.subplot( 3, 2, 5 )\n",
        "sns.countplot( df4['assortment'] )\n",
        "\n",
        "plt.subplot( 3, 2, 6 )\n",
        "sns.kdeplot( df4[df4['assortment'] == 'extended']['sales'], label='extended', shade=True )\n",
        "sns.kdeplot( df4[df4['assortment'] == 'basic']['sales'], label='basic', shade=True )\n",
        "sns.kdeplot( df4[df4['assortment'] == 'extra']['sales'], label='extra', shade=True )\n",
        "sns.kdeplot( df4[df4['assortment'] == 'd']['sales'], label='d', shade=True )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIIAH779s09U"
      },
      "source": [
        "## 4.2. ANALISE BIVARIADA ( Encontrar o impacto de uma variável em relação a variável resposta )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osb6FsCT_rhM"
      },
      "source": [
        "### H1. Lojas com maior sortimentos deveriam vender mais.\n",
        "### ***FALSA*** Lojas com maior sortimento vendem menos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wItUxTEoAPSJ"
      },
      "outputs": [],
      "source": [
        "#Quantidade de venda por tipo de assortment\n",
        "aux1 = df4[['assortment', 'sales']].groupby( 'assortment' ).sum().reset_index() \n",
        "sns.barplot( x='assortment', y='sales', data=aux1 );\n",
        "\n",
        "aux2 = df4[[ 'year_week', 'assortment', 'sales']].groupby( ['year_week','assortment'] ).sum().reset_index()\n",
        "aux2.pivot( index='year_week', columns='assortment', values='sales' ).plot()\n",
        "\n",
        "aux3 = aux2[aux2['assortment'] == 'extra' ]\n",
        "aux3.pivot( index='year_week', columns='assortment', values='sales' ).plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFMt96V3bSVO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqHndotYAGNi"
      },
      "source": [
        "### H2. Lojas com competidores mais próximos deveriam vender menos.\n",
        "### ***FALSA*** Lojas com COMPETIDORES MAIS PROXIMOS vendem MAIS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX4clqzfZvSR"
      },
      "outputs": [],
      "source": [
        "plt.rcParams.update({'figure.figsize':(30,15), 'figure.dpi':100})\n",
        "\n",
        "aux1 = df4[['competition_distance', 'sales']].groupby( 'competition_distance' ).sum().reset_index()\n",
        "\n",
        "plt.subplot( 1, 3, 1 )\n",
        "sns.scatterplot( x ='competition_distance', y='sales', data=aux1 );\n",
        "\n",
        "plt.subplot( 1, 3, 2 )\n",
        "bins = list( np.arange( 0, 20000, 1000 ) )\n",
        "aux1['competition_distance_binned'] = pd.cut( aux1['competition_distance'], bins=bins )\n",
        "aux2 = aux1[['competition_distance_binned', 'sales']].groupby( 'competition_distance_binned' ).sum().reset_index()\n",
        "sns.barplot( x='competition_distance_binned', y='sales', data=aux2 );\n",
        "plt.xticks( rotation=90 );\n",
        "\n",
        "plt.subplot( 1, 3, 3 )\n",
        "sns.heatmap( aux1.corr( method='pearson' ), annot=True );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3utuDePAGKc"
      },
      "source": [
        "### H3. Lojas com competidores à mais tempo deveriam vender mais.\n",
        "### ***FALSA*** Lojas com COMPETIDORES À MAIS TEMPO vendem MENOS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6beFHb_LdVk"
      },
      "outputs": [],
      "source": [
        "plt.subplot(  1, 3, 1 )\n",
        "aux1 = df4[['competition_time_month', 'sales']].groupby( 'competition_time_month' ).sum().reset_index()\n",
        "aux2 = aux1[( aux1['competition_time_month'] < 120 ) & ( aux1['competition_time_month'] != 0 )]\n",
        "sns.barplot( x='competition_time_month', y='sales', data=aux2 );\n",
        "plt.xticks( rotation=90 );\n",
        "\n",
        "plt.subplot(  1, 3, 2 )\n",
        "sns.regplot( x='competition_time_month', y='sales', data=aux2 );\n",
        "\n",
        "plt.subplot(  1, 3, 3 )\n",
        "x = sns.heatmap( aux1.corr( method='pearson'), annot=True );\n",
        "bottom, top = x.get_ylim()\n",
        "x.set_ylim( bottom+0.5, top-0.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBmEtrqlAGHk"
      },
      "source": [
        "### H4. Lojas com promoções ativas por mais tempo deveriam vender mais.\n",
        "### ***FALSA*** Lojas com promoções ativas por mais tempo, vendem menos depois de um certo período de promoção."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O40Ntj1-Vkhm"
      },
      "outputs": [],
      "source": [
        "aux1 = df4[['promo_time_week', 'sales']].groupby( 'promo_time_week').sum().reset_index()\n",
        "\n",
        "grid = GridSpec( 2, 3 )\n",
        "\n",
        "plt.subplot( grid[0,0] )\n",
        "aux2 = aux1[aux1['promo_time_week'] > 0] # promo extendida\n",
        "sns.barplot( x='promo_time_week' , y='sales', data=aux2 );\n",
        "plt.xticks( rotation=90 );\n",
        "\n",
        "plt.subplot( grid[0,1] )\n",
        "sns.regplot( x='promo_time_week' , y='sales', data=aux2 );\n",
        "\n",
        "plt.subplot( grid[1,0] )\n",
        "aux3 = aux1[aux1['promo_time_week'] < 0] # promo regular\n",
        "sns.barplot( x='promo_time_week' , y='sales', data=aux3 );\n",
        "plt.xticks( rotation=90 );\n",
        "\n",
        "plt.subplot( grid[1,1] )\n",
        "sns.regplot( x='promo_time_week' , y='sales', data=aux3 );\n",
        "\n",
        "plt.subplot( grid[:,2] )\n",
        "sns.heatmap( aux1.corr(method='pearson' ), annot=True );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGHeenp1AGEk"
      },
      "source": [
        "### H5. Lojas com mais dias de promoção deveriam vender mais. #Validar no próximo ciclo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wcb63r9AGB2"
      },
      "source": [
        "### H7. Lojas com mais promoções consecutivas deveriam vender mais.\n",
        "### ***FALSA*** Lojas com mais promoções consecutivas vendem menos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cqAbNaO4Zgt"
      },
      "outputs": [],
      "source": [
        "df4[['promo', 'promo2', 'sales']].groupby( ['promo', 'promo2' ] ).sum().reset_index() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDzA9-v55XvY"
      },
      "outputs": [],
      "source": [
        "aux1 = df4[( df4['promo'] == 1 ) & ( df4['promo2'] == 1 )][['year_week','sales']].groupby( 'year_week' ).sum().reset_index()\n",
        "ax = aux1.plot()\n",
        "\n",
        "aux2 = df4[( df4['promo'] == 1 ) & ( df4['promo2'] == 0 )][['year_week','sales']].groupby( 'year_week' ).sum().reset_index()\n",
        "aux2.plot( ax=ax )\n",
        "\n",
        "ax.legend( labels=['Tradicional & Extendida', 'Extendida']);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV3e7IBfAF7i"
      },
      "source": [
        "### H8. Lojas abertas durante o feriado de Natal deveriam vender mais.\n",
        "### ***FALSA*** Lojas abertas durante o feriado do natal vendem menos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyndMXeot9vI"
      },
      "outputs": [],
      "source": [
        "aux = df4[df4['state_holiday'] != 'regular_day']\n",
        "\n",
        "plt.subplot( 1, 2, 1 )\n",
        "aux1 = aux[['state_holiday', 'sales']].groupby( 'state_holiday' ).sum().reset_index()\n",
        "sns.barplot( x='state_holiday', y='sales', data=aux1 );\n",
        "\n",
        "plt.subplot( 1, 2, 2 )\n",
        "aux2 = aux[['year', 'state_holiday', 'sales']].groupby( ['year', 'state_holiday'] ).sum().reset_index()\n",
        "sns.barplot( x='year' , y='sales', hue='state_holiday', data=aux2 );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7f6Pby9AF0J"
      },
      "source": [
        "### H9. Lojas deveriam vender mais ao longo dos anos.\n",
        "### ***FALSA*** Lojas vendem menos ao longo dos anos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlAxsRNFVHQR"
      },
      "outputs": [],
      "source": [
        "aux1 = df4[['year', 'sales']].groupby( 'year' ).sum().reset_index()\n",
        "\n",
        "plt.subplot( 1, 3, 1 )\n",
        "sns.barplot( x='year', y='sales', data=aux1 );\n",
        "\n",
        "plt.subplot( 1, 3, 2 )\n",
        "sns.regplot( x='year', y='sales', data=aux1 ); #Gráfico de tendêcia\n",
        "\n",
        "plt.subplot( 1, 3, 3 )\n",
        "sns.heatmap( aux1.corr( method='pearson' ), annot=True ); #Gráfico de correlação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKaGxcjgAF-2"
      },
      "source": [
        "### H10. Lojas deveriam vender mais no segundo semestre do ano.\n",
        "### ***FALSA*** Lojas vendem menos no segundo semestre do ano."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J7WLcOHZC8f"
      },
      "outputs": [],
      "source": [
        "aux1 = df4[['month', 'sales']].groupby( 'month' ).sum().reset_index()\n",
        "\n",
        "plt.subplot( 1, 3, 1 )\n",
        "sns.barplot( x='month', y='sales', data=aux1 );\n",
        "\n",
        "plt.subplot( 1, 3, 2 )\n",
        "sns.regplot( x='month', y='sales', data=aux1 ); #Gráfico de tendêcia\n",
        "\n",
        "plt.subplot( 1, 3, 3 )\n",
        "sns.heatmap( aux1.corr( method='pearson' ), annot=True ); #Gráfico de correlação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhreIpfEtfjf"
      },
      "source": [
        "### H11. Lojas deveriam vender mais depois do dia 10 de cada mês.\n",
        "### ***VERDADEIRA*** Lojas vendem menos no segundo semestre do ano."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAmzvJGIZYb0"
      },
      "outputs": [],
      "source": [
        "aux1 = df4[['day', 'sales']].groupby( 'day' ).sum().reset_index()\n",
        "\n",
        "plt.subplot( 2, 2, 1 )\n",
        "sns.barplot( x='day', y='sales', data=aux1 );\n",
        "\n",
        "plt.subplot( 2, 2, 2 )\n",
        "sns.regplot( x='day', y='sales', data=aux1 ); #Gráfico de tendêcia\n",
        "\n",
        "plt.subplot( 2, 2, 3 )\n",
        "sns.heatmap( aux1.corr( method='pearson' ), annot=True ); #Gráfico de correlação\n",
        "\n",
        "aux1['before_after'] = aux1['day'].apply( lambda x: 'before_10_days' if x <= 10 else 'after_10_days' )\n",
        "aux2 = aux1[['before_after', 'sales']].groupby( 'before_after' ).sum().reset_index()\n",
        "\n",
        "plt.subplot( 2, 2, 4 )\n",
        "sns.barplot( x='before_after', y='sales', data=aux2 );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDBxiOfztfWc"
      },
      "source": [
        "### H12. Lojas deveriam vender menos aos finais de semana.\n",
        "### ***VERDADEIRA*** Lojas vendem menos nos finais de semana."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q45rLpvmb7S-"
      },
      "outputs": [],
      "source": [
        "aux1 = df4[['day_of_week', 'sales']].groupby( 'day_of_week' ).sum().reset_index()\n",
        "\n",
        "plt.subplot( 1, 3, 1 )\n",
        "sns.barplot( x='day_of_week', y='sales', data=aux1 );\n",
        "\n",
        "plt.subplot( 1, 3, 2 )\n",
        "sns.regplot( x='day_of_week', y='sales', data=aux1 ); #Gráfico de tendêcia\n",
        "\n",
        "plt.subplot( 1, 3, 3 )\n",
        "sns.heatmap( aux1.corr( method='pearson' ), annot=True ); #Gráfico de correlação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4hPY7gWt3qV"
      },
      "source": [
        "### H13. Lojas deveriam vender menos durante os feriados escolares.\n",
        "### ***VERDADEIRA*** Lojas vendem menos durante os feriados escolares, exceto os meses de julho e agosto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viio8dRbdGsd"
      },
      "outputs": [],
      "source": [
        "aux1 = df4[[ 'school_holiday', 'sales']].groupby( 'school_holiday' ).sum().reset_index()\n",
        "\n",
        "plt.subplot( 2, 1, 1 )\n",
        "sns.barplot( x='school_holiday', y='sales', data=aux1 );\n",
        "\n",
        "plt.subplot( 2, 1, 2 )\n",
        "aux2 = df4[['month', 'school_holiday', 'sales']].groupby( ['month','school_holiday'] ).sum().reset_index()\n",
        "sns.barplot( x='month', y='sales', hue='school_holiday', data=aux2 );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1Sqpjqknpz-"
      },
      "source": [
        "## 4.2.1. Resumo das hipóteses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ePSJ9DEn4zn"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVlLf0VMoBVV"
      },
      "outputs": [],
      "source": [
        "tab=[[' Hipoteses', 'Conclusao', 'Relevancia'],\n",
        "     ['H1', 'Falsa', 'Baixa'],\n",
        "     ['H2', 'Falsa', 'Media'],\n",
        "     ['H3', 'Falsa', 'Media'],\n",
        "     ['H4', 'Falsa', 'Baixa'],\n",
        "     ['H5', '_', '_'],\n",
        "     ['H7', 'Falsa', 'Baixa'],\n",
        "     ['H8', 'Falsa', 'Media'],\n",
        "     ['H9', 'Falsa', 'Alta'],\n",
        "     ['H10', 'Falsa', 'Alta'],\n",
        "     ['H11', 'Verdadeira', 'Alta'],\n",
        "     ['H12', 'Verdadeira', 'Alta'],\n",
        "     ['H13', 'Verdadeira', 'Baixa'],\n",
        "    ]\n",
        "print( tabulate( tab, headers='firstrow' ) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHR4BxhwszJZ"
      },
      "source": [
        "## 4.3. ANALISE MULTIVARIADA  ( Encontrar as correlações entre as próprias variáveis )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-cSDG6wt02R"
      },
      "source": [
        "### 4.3.1 Numerical Attributes ( Variáveis numéricas -> método de Pearson )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d9SEhe-xh-g"
      },
      "outputs": [],
      "source": [
        "correlation = num_attributes.corr( method='pearson' )\n",
        "sns.heatmap( correlation, annot=True );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BS4fErVt_mO"
      },
      "source": [
        "### 4.3.1 Categorical Attributes ( Variáveis Categóricas -> método V de Cramer )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qff-2v2JyTFx"
      },
      "outputs": [],
      "source": [
        "# only categorical data\n",
        "a = df4.select_dtypes( include='object' )\n",
        "\n",
        "# Calculate cramer V\n",
        "a1 = cramer_v(a['state_holiday'], a['state_holiday'] ) \n",
        "a2 = cramer_v(a['state_holiday'], a['store_type'] ) \n",
        "a3 = cramer_v(a['state_holiday'], a['assortment'] )\n",
        "\n",
        "a4 = cramer_v(a['store_type'], a['state_holiday'] )\n",
        "a5 = cramer_v(a['store_type'], a['store_type'] )\n",
        "a6 = cramer_v(a['store_type'], a['assortment'] )\n",
        "\n",
        "a7 = cramer_v(a['assortment'], a['state_holiday'] )\n",
        "a8 = cramer_v(a['assortment'], a['store_type'] )\n",
        "a9 = cramer_v(a['assortment'], a['assortment'] )\n",
        "\n",
        "# Final dataset\n",
        "d = pd.DataFrame( {'state_holiday': [a1, a2, a3], \n",
        "               'store_type': [a4, a5, a6], \n",
        "               'assortment': [a7, a8, a9] })\n",
        "\n",
        "d = d.set_index( d.columns )\n",
        "\n",
        "sns.heatmap( d, annot=True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoBuvjNi_t0_"
      },
      "source": [
        "# 5.0. PASSO 05 - DATA PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTYxI2vSCYJ2"
      },
      "outputs": [],
      "source": [
        "df5 = df4.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHvp6W8oCfP-"
      },
      "source": [
        "## 5.1. Normalização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvKvzhu8CeAC"
      },
      "source": [
        "## 5.2. Rescaling ( Normalizando os ranges  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLdMEAOIF7Ke"
      },
      "outputs": [],
      "source": [
        "rs = RobustScaler() # Aplicado em variáveis com Outliers fortes\n",
        "mms = MinMaxScaler() # Aplicado em variáveis sem Outliers fortes\n",
        "\n",
        "# competition distance\n",
        "df5['competition_distance'] = rs.fit_transform( df5[['competition_distance']].values )\n",
        "pickle.dump( rs, open( '/content/competition_distance_scaler.pkl', 'wb') )\n",
        "files.download('/content/competition_distance_scaler.pkl')  \n",
        "# competition time month\n",
        "df5['competition_time_month'] = rs.fit_transform( df5[['competition_time_month']].values )\n",
        "pickle.dump( rs, open( '/content/competition_time_month_scaler.pkl','wb') )\n",
        "files.download('/content/competition_time_month_scaler.pkl') \n",
        "# promo time week\n",
        "df5['promo_time_week'] = mms.fit_transform( df5[['promo_time_week']].values )\n",
        "pickle.dump( rs, open( '/content/promo_time_week_scaler.pkl','wb') )\n",
        "files.download('/content/promo_time_week_scaler.pkl') \n",
        "# year\n",
        "df5['year'] = mms.fit_transform( df5[['year']].values )\n",
        "pickle.dump( mms, open( '/content/year_scaler.pkl','wb') )\n",
        "files.download('/content/year_scaler.pkl') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsUcwLrSpbmZ"
      },
      "outputs": [],
      "source": [
        "sns.displot( df5['competition_distance'] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bvJiBgiCd9M"
      },
      "source": [
        "## 5.3. Transformação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agpnBKG-YY0l"
      },
      "source": [
        "### 5.3.1. Enconding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0gUvkXVzOWa"
      },
      "outputs": [],
      "source": [
        "# state_holiday ( One Hot Enconding )\n",
        "df5 = pd.get_dummies( df5, prefix=['state_holiday'], columns=['state_holiday'] )\n",
        "\n",
        "# store_type ( Label Encoding )\n",
        "le = LabelEncoder()\n",
        "df5['store_type'] = le.fit_transform( df5['store_type'] )\n",
        "pickle.dump( le, open( '/content/store_type_scaler.pkl','wb') )\n",
        "files.download('/content/store_type_scaler.pkl')\n",
        "\n",
        "# assortment ( Ordinal Encoding )\n",
        "assortment_dict = {'basic': 1, 'extra': 2, 'extended': 3}\n",
        "df5['assortment'] = df5['assortment'].map( assortment_dict )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m_ucl1t3ncQ"
      },
      "source": [
        "### 5.3.2. Response Variable Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQoB5IaC37f4"
      },
      "outputs": [],
      "source": [
        "df5['sales'] = np.log1p(df5['sales'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmwJE1zi4XwO"
      },
      "outputs": [],
      "source": [
        "sns.distplot( df5['sales'] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJQ4NRCF8oJu"
      },
      "source": [
        "### 5.3.3. Nature Transformation ( Natureza Cíclica )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fzlv5EkICsj_"
      },
      "outputs": [],
      "source": [
        "# month\n",
        "df5['month_sin'] = df5['month'].apply( lambda x: np.sin( x* ( 2. * np.pi/12 ) ) )\n",
        "df5['month_cos'] = df5['month'].apply( lambda x: np.cos( x* ( 2. * np.pi/12 ) ) )\n",
        "\n",
        "# day\n",
        "df5['day_sin'] = df5['day'].apply( lambda x: np.sin( x* ( 2. * np.pi/30 ) ) )\n",
        "df5['day_cos'] = df5['day'].apply( lambda x: np.cos( x* ( 2. * np.pi/30 ) ) )\n",
        "\n",
        "# week of year\n",
        "df5['week_of_year_sin'] = df5['week_of_year'].apply( lambda x: np.sin( x* ( 2. * np.pi/52 ) ) )\n",
        "df5['week_of_year_cos'] = df5['week_of_year'].apply( lambda x: np.cos( x* ( 2. * np.pi/52 ) ) )\n",
        "\n",
        "# day of week\n",
        "df5['day_of_week_sin'] = df5['day_of_week'].apply( lambda x: np.sin( x* ( 2. * np.pi/7 ) ) )\n",
        "df5['day_of_week_cos'] = df5['day_of_week'].apply( lambda x: np.cos( x* ( 2. * np.pi/7 ) ) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYWDzDO1de7I"
      },
      "source": [
        "# 6.0. PASSO 06 - FEATURE SELECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJKYzui0-ZT-"
      },
      "outputs": [],
      "source": [
        "df6 = df5.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9cSWGlMdvMZ"
      },
      "source": [
        "## 6.1. Split dataframe into training and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo0EBaHbedxt"
      },
      "outputs": [],
      "source": [
        "cols_drop = [ 'week_of_year', 'day', 'month', 'day_of_week', 'promo_since', 'competition_since', 'year_week' ]\n",
        "df6 = df6.drop( cols_drop, axis=1 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch7yEAzGfF5z"
      },
      "outputs": [],
      "source": [
        "#df6[['store', 'date']].groupby( 'store' ).max().reset_index()['date'][0] - datetime.timedelta( days=6*7 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uKtftvLzygr"
      },
      "outputs": [],
      "source": [
        "# training dataset\n",
        "X_train = df6[df6['date'] < '2015-06-19' ]\n",
        "y_train = X_train['sales']\n",
        "\n",
        "# test dataset\n",
        "X_test = df6[df6['date'] >= '2015-06-19' ]\n",
        "y_test = X_test['sales']\n",
        "\n",
        "print( 'Training Min Date: {}'.format( X_train['date'].min() ) )\n",
        "print( 'Training Max Date: {}'.format( X_train['date'].max() ) )\n",
        "\n",
        "print( '\\nTest Min Date: {}'.format( X_test['date'].min() ) )\n",
        "print( 'Test Max Date: {}'.format( X_test['date'].max() ) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R72pWFnyd9ZE"
      },
      "source": [
        "## 6.2. Boruta as Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1O1TBnzdtHf"
      },
      "outputs": [],
      "source": [
        "## training and test dataset for Boruta\n",
        "# X_train_n = X_train.drop( ['date', 'sales'], axis=1 ).values\n",
        "# y_train_n = y_train.values.ravel()\n",
        "\n",
        "## define RandomForestRegressor\n",
        "# rf = RandomForestRegressor( n_jobs=-1)\n",
        "\n",
        "## define Boruta\n",
        "# boruta = BorutaPy( rf, n_estimators='auto', verbose=2, random_state=42 ).fit( X_train_n, y_train_n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezN_erKE7aOy"
      },
      "source": [
        "### 6.2.1. Best Features from Boruta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jrk6lneg5dLU"
      },
      "outputs": [],
      "source": [
        "# cols_selected = boruta.support_.tolist()\n",
        "\n",
        "## best features\n",
        "# X_train_fs = X_train.drop( ['date', 'sales'], axis=1 )\n",
        "# cols_selected_boruta = X_train_fs.iloc[:, cols_selected].columns.to_list()\n",
        "\n",
        "# not selected boruta\n",
        "# cols_not_selected_boruta = list( np.setdiff1d( X_train_fs.columns, cols_selected_boruta ) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAUCjBXkpzKG"
      },
      "source": [
        "### 6.3. Manual Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFJlWwMlfH-W"
      },
      "outputs": [],
      "source": [
        "cols_selected_boruta = [\n",
        "    'store',\n",
        "    'promo',\n",
        "    'store_type',\n",
        "    'assortment',\n",
        "    'competition_distance',\n",
        "    'competition_open_since_month',\n",
        "    'competition_open_since_year',\n",
        "    'promo2',\n",
        "    'promo2_since_week',\n",
        "    'promo2_since_year',\n",
        "    'competition_time_month',\n",
        "    'promo_time_week',\n",
        "    'month_sin',\n",
        "    'month_cos',\n",
        "    'day_sin',\n",
        "    'day_cos',\n",
        "    'week_of_year_sin',\n",
        "    'week_of_year_cos',\n",
        "    'day_of_week_sin',\n",
        "    'day_of_week_cos']\n",
        "\n",
        "# columns to add\n",
        "feat_to_add = ['date', 'sales']\n",
        "\n",
        "# final features\n",
        "\n",
        "cols_selected_boruta_full = cols_selected_boruta.copy()\n",
        "cols_selected_boruta_full.extend( feat_to_add )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6UAKnOlq_do"
      },
      "source": [
        "# 7.0. PASSO 07 - MACHINE LEARNING MODELING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8oHZ2NlVD9O"
      },
      "outputs": [],
      "source": [
        "x_train = X_train[ cols_selected_boruta ]\n",
        "x_test = X_test[ cols_selected_boruta ]\n",
        "\n",
        "# Time Series Data Preparation\n",
        "x_training = X_train[ cols_selected_boruta_full ] # todas as variáveis relevantes + date + sales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ooXreuhUdQt"
      },
      "source": [
        "## 7.1. Average Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFlC1EXfNx8w"
      },
      "outputs": [],
      "source": [
        "aux1 = x_test.copy()\n",
        "aux1['sales'] = y_test.copy()\n",
        "\n",
        "# prediction\n",
        "aux2 = aux1[['store', 'sales']].groupby( 'store' ).mean().reset_index().rename( columns={'sales': 'predictions'} )\n",
        "aux1 = pd.merge( aux1, aux2, how='left', on='store' )\n",
        "yhat_baseline = aux1['predictions']\n",
        "\n",
        "# performance\n",
        "baseline_result = ml_error( 'Average Model', np.expm1( y_test ), np.expm1( yhat_baseline ) )\n",
        "baseline_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHqRjzSbUnGv"
      },
      "source": [
        "## 7.2. Linear Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcNsrfGxNyA9"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "lr = LinearRegression().fit( x_train, y_train )\n",
        "\n",
        "# prediction\n",
        "yhat_lr = lr.predict( x_test )\n",
        "\n",
        "# performance\n",
        "lr_result = ml_error( 'Linear Regression', np.expm1( y_test ), np.expm1( yhat_lr ) )\n",
        "lr_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjw9QbM9UrnN"
      },
      "source": [
        "### 7.2.1. Linear Regression Model - Cross Validation\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mDeH6hzNx-v"
      },
      "outputs": [],
      "source": [
        "lr_result_cv = cross_validation( x_training, 5, 'Linear Regression', lr )\n",
        "lr_result_cv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epUmwLzMtGIg"
      },
      "source": [
        "## 7.3. Linear Regression Regularized Model - Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1U7Vow9Nx6f"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "lrr = Lasso( alpha=0.01 ).fit( x_train, y_train )\n",
        "\n",
        "# prediction\n",
        "yhat_lrr = lrr.predict( x_test )\n",
        "\n",
        "# performance\n",
        "lrr_result = ml_error( 'Linear Regression - Lasso', np.expm1( y_test ), np.expm1( yhat_lrr ) )\n",
        "lrr_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixnopyV9uQFg"
      },
      "source": [
        "### 7.3.1. Lasso - Cross Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHL-AAemNx4V"
      },
      "outputs": [],
      "source": [
        "lrr_result_cv = cross_validation( x_training, 5, 'Lasso', lrr )\n",
        "lrr_result_cv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J95fe9-7y9S"
      },
      "source": [
        "## 7.4. Random Forest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3-mH7pXNx1f"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "rf = RandomForestRegressor( n_estimators=100, n_jobs=-1, random_state=42 ).fit( x_train, y_train )\n",
        "\n",
        "# prediction\n",
        "yhat_rf = rf.predict( x_test )\n",
        "\n",
        "# performance\n",
        "rf_result = ml_error( 'Random Forest Regressor', np.expm1( y_test ), np.expm1( yhat_rf ) )\n",
        "rf_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKVO7wRBhIQ_"
      },
      "source": [
        "### 7.4.1. Random Forest Regressor Model - Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKmYTU-ahOKw"
      },
      "outputs": [],
      "source": [
        "rf_result_cv = cross_validation( x_training, 5, 'Random Forest Regressor', rf )\n",
        "rf_result_cv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOqFh1Bh83Nu"
      },
      "source": [
        "## 7.5. XGBoost Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zKDgWy0Nxzw"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "model_xgb = xgb.XGBRegressor( objective='reg:squarederror',\n",
        "                              n_estimators=100,\n",
        "                              eta=0.01,\n",
        "                              max_depth=10,\n",
        "                              subsample=0.7,\n",
        "                              colsample_bytee=0.9 ).fit( x_train, y_train )\n",
        "\n",
        "# prediction\n",
        "yhat_xgb = model_xgb.predict( x_test )\n",
        "\n",
        "# performance\n",
        "xgb_result = ml_error( 'XGBoost Regressor', np.expm1( y_test ), np.expm1( yhat_xgb ) )\n",
        "xgb_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWQ6OeleiIGk"
      },
      "source": [
        "### 7.5.1. XGBoost Regressor Model - Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rKJ-461iIfW"
      },
      "outputs": [],
      "source": [
        "xgb_result_cv = cross_validation( x_training, 5, 'XGBoost Regressor', model_xgb )\n",
        "xgb_result_cv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH95WOaTU6hu"
      },
      "source": [
        "## 7.6. Compare Model's Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y99H_AUAip9t"
      },
      "source": [
        "### 7.6.1. Single Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kN39OJMNxxb"
      },
      "outputs": [],
      "source": [
        "modelling_result = pd.concat( [baseline_result, lr_result, lrr_result, rf_result, xgb_result] )\n",
        "modelling_result.sort_values( 'RMSE' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcyWoDPiivm0"
      },
      "source": [
        "### 7.6.1. Real Performance - Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kub6MLyCis-8"
      },
      "outputs": [],
      "source": [
        "modelling_result_cv = pd.concat( [lr_result_cv, lrr_result_cv, rf_result_cv, xgb_result_cv] )\n",
        "modelling_result_cv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l53P6u6ZDKUm"
      },
      "source": [
        "# 8.0. PASSO 08 - HYPERPARAMETER FINE TUNING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2W5uJzrf7rX"
      },
      "source": [
        "## 8.1. Random Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXmBZi0Hg62a"
      },
      "outputs": [],
      "source": [
        "#param = {\n",
        "#    'n_estimators': [1500, 1700, 2500, 3000, 3500],\n",
        "#    'eta': [0.01, 0.03],\n",
        "#    'max_depth': [3, 5, 9],\n",
        "#    'subsample': [0.1, 0.5, 0.7],\n",
        "#    'colsample_bytee': [0.3, 0.7, 0.9],\n",
        "#    'min_child_weight': [3, 8, 15]\n",
        "#         }\n",
        "#MAX_EVAL = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FpGDfveDfGz"
      },
      "outputs": [],
      "source": [
        "#final_result = pd.DataFrame()\n",
        "\n",
        "#for i in range( MAX_EVAL ):\n",
        "#    # choose values for parameters randomly\n",
        "#    hp = { k: np.random.choice( v, 1 )[0] for k, v in param.items() }\n",
        "#    print( hp )\n",
        "\n",
        "#    # model\n",
        "#    model_xgb = xgb.XGBRegressor( objective='reg:squarederror',\n",
        "#                                n_estimators=hp['n_estimators'],\n",
        "#                                eta=hp['eta'],\n",
        "#                                max_depth=hp['max_depth'],\n",
        "#                                subsample=hp['subsample'],\n",
        "#                                colsample_bytee=hp['colsample_bytee'],\n",
        "#                                min_child_weight=hp['min_child_weight'] )\n",
        "    \n",
        "#    # performance\n",
        "#    result = cross_validation( x_training, 2, 'XGBoost Regressor', model_xgb, verbose=False )\n",
        "#    final_result = pd.concat( [final_result, result] )\n",
        "\n",
        "#final_result "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjQAZPoFgEf3"
      },
      "source": [
        "## 8.1. Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WuxsRJ4f3na"
      },
      "outputs": [],
      "source": [
        "param_tuned = {\n",
        "    'n_estimators': 3000,\n",
        "    'eta': 0.03,\n",
        "    'max_depth': 5,\n",
        "    'subsample': 0.7,\n",
        "    'colsample_bytee': 0.7,\n",
        "    'min_child_weight': 3\n",
        "         }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWF5V_pxf3ko"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "model_xgb_tuned = xgb.XGBRegressor( objective='reg:squarederror',\n",
        "                                    n_estimators=param_tuned['n_estimators'],\n",
        "                                    eta=param_tuned['eta'],\n",
        "                                    max_depth=param_tuned['max_depth'],\n",
        "                                    subsample=param_tuned['subsample'],\n",
        "                                    colsample_bytee=param_tuned['colsample_bytee'],\n",
        "                                    min_child_weight=param_tuned['min_child_weight'] ).fit( x_train, y_train )\n",
        "\n",
        "# prediction\n",
        "yhat_xgb_tuned = model_xgb_tuned.predict( x_test )\n",
        "    \n",
        "# performance\n",
        "xgb_result_tuned = ml_error( 'XGBoost Regressor', np.expm1( y_test ), np.expm1( yhat_xgb_tuned ) )\n",
        "xgb_result_tuned"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3. Download do modelo final"
      ],
      "metadata": {
        "id": "Cc6QxrryIaUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.0. PASSO 09 - TRADUÇÃO E INTERPRETEÇÃO DO ERRO"
      ],
      "metadata": {
        "id": "lauhv_lWiWTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df9 = X_test[ cols_selected_boruta_full ]\n",
        "\n",
        "# rescale\n",
        "df9['sales'] = np.expm1( df9['sales'] )\n",
        "df9['predictions'] = np.expm1( yhat_xgb_tuned  )"
      ],
      "metadata": {
        "id": "XOCGT4aziiiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.1. Business Performance"
      ],
      "metadata": {
        "id": "iYbOmkZHilU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sum of predictions\n",
        "df91 = df9[['store', 'predictions']].groupby('store' ).sum().reset_index()\n",
        "\n",
        "# MAE and MAPE\n",
        "df9_aux1 = df9[['store', 'sales', 'predictions']].groupby( 'store' ).apply( lambda x: mean_absolute_error( x['sales'], x['predictions'] ) ).reset_index().rename(columns={0:'MAE'} )\n",
        "df9_aux2 = df9[['store', 'sales', 'predictions']].groupby( 'store' ).apply( lambda x: mean_absolute_percentage_error( x['sales'], x['predictions'] ) ).reset_index().rename(columns={0:'MAPE'} )\n",
        "\n",
        "# Merge\n",
        "df9_aux3 = pd.merge( df9_aux1, df9_aux2, how='inner', on='store' )\n",
        "df92 = pd.merge( df91, df9_aux3, how='inner', on='store' )\n",
        "\n",
        "# Scenarios\n",
        "df92['worst_scenario'] = df92['predictions'] - df92['MAE']\n",
        "df92['best_scenario'] = df92['predictions'] + df92['MAE']\n",
        "\n",
        "# order columns\n",
        "df92 = df92[[ 'store', 'predictions', 'worst_scenario', 'best_scenario', 'MAE', 'MAPE']]"
      ],
      "metadata": {
        "id": "TjIUcE-4io81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df92.sample(10)"
      ],
      "metadata": {
        "id": "OU5FjhcmlPND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df92.sort_values( 'MAPE', ascending=False ).head()"
      ],
      "metadata": {
        "id": "Hot7B6ezoklL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot( x='store', y='MAPE', data=df92 )"
      ],
      "metadata": {
        "id": "3f6XXoT6o51F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2. Total Performance"
      ],
      "metadata": {
        "id": "wKoboWOLipaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df93 = df92[['predictions', 'worst_scenario', 'best_scenario']].apply( lambda x: np.sum( x ), axis=0 ).reset_index().rename( columns={'index': 'Scenario', 0: 'Values'} )\n",
        "df93['Values'] = df93['Values'].map( 'R${:,.2f}'.format )\n",
        "df93"
      ],
      "metadata": {
        "id": "C170HJKaitqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.3. Machine Learning Performance"
      ],
      "metadata": {
        "id": "lxvl9H17iuMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df9['error'] = df9['sales'] - df9['predictions']\n",
        "df9['error_rate'] = df9['predictions'] / df9['sales']"
      ],
      "metadata": {
        "id": "Rrz-rGjcizD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot( 2, 2, 1 )\n",
        "sns.lineplot( x='date', y='sales', data=df9, label='SALES' )\n",
        "sns.lineplot( x='date', y='predictions', data=df9, label='PREDICTIONS' )\n",
        "\n",
        "plt.subplot( 2, 2, 2 )\n",
        "sns.lineplot( x='date', y='error_rate', data=df9 )\n",
        "plt.axhline( 1, linestyle='--' )\n",
        "\n",
        "plt.subplot( 2, 2, 3 )\n",
        "sns.distplot( df9['error'] )\n",
        "\n",
        "plt.subplot( 2, 2, 4 )\n",
        "sns.scatterplot( df9['predictions'], df9['error'] )"
      ],
      "metadata": {
        "id": "1Us4P6gH0b-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.0. PASSO 10 - DEPLOY MODEL TO PRODUCTION"
      ],
      "metadata": {
        "id": "i_ciuWxqS8Fm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgMzPGeYf3iU"
      },
      "outputs": [],
      "source": [
        "# Saving Trained Model\n",
        "\n",
        "pickle.dump( model_xgb_tuned, open(\"/content/Rossmann_Store_Sales/Model trained/model_rossmann.pkl\", 'wb' ) )\n",
        "#from google.colab import files\n",
        "#files.download('/content/model_rossmann.pkl') "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.1. Rossmann Class"
      ],
      "metadata": {
        "id": "yJE2gJnUTSrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import inflection\n",
        "import numpy as np\n",
        "import math\n",
        "import datetime\n",
        "\n",
        "class Rossmann( object ):\n",
        "    def __init__( self ):\n",
        "        self.home_path='/content/Rossmann_Store_Sales/'\n",
        "        self.competition_distance_scaler      = pickle.load( open( self.home_path + 'Parameter/competition_distance_scaler.pkl', 'rb') )\n",
        "        self.promo_time_week_scaler           = pickle.load( open( self.home_path + 'Parameter/promo_time_week_scaler.pkl','rb') )\n",
        "        self.competition_time_month_scaler    = pickle.load( open( self.home_path + 'Parameter/competition_time_month_scaler.pkl','rb') )\n",
        "        self.year_scaler                      = pickle.load( open( self.home_path + 'Parameter/year_scaler.pkl','rb') )\n",
        "        self.store_type_scaler                = pickle.load( open( self.home_path + 'Parameter/store_type_scaler.pkl','rb') )\n",
        "          \n",
        "        \n",
        "\n",
        "    def data_cleaning( self, df1 ):\n",
        "\n",
        "        ## 1.1. Rename Columns\n",
        "\n",
        "        cols_old = ['Store', 'DayOfWeek', 'Date', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', \n",
        "                    'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
        "                    'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
        "\n",
        "        snakecase = lambda x: inflection.underscore( x )\n",
        "\n",
        "        cols_new = list( map( snakecase, cols_old ) )\n",
        "\n",
        "        # rename\n",
        "        df1.columns = cols_new\n",
        "\n",
        "        ## 1.3. Data Types\n",
        "        df1['date'] = pd.to_datetime( df1['date'] )\n",
        "\n",
        "        ## 1.5. Fillout NA\n",
        "        #competition_distance        \n",
        "        df1['competition_distance'] = df1['competition_distance'].apply( lambda x: 200000.0 if math.isnan( x ) else x )\n",
        "\n",
        "        #competition_open_since_month\n",
        "        df1['competition_open_since_month'] = df1.apply( lambda x: x['date'].month if math.isnan( x['competition_open_since_month'] ) else x['competition_open_since_month'], axis=1 )\n",
        "\n",
        "        #competition_open_since_year \n",
        "        df1['competition_open_since_year'] = df1.apply( lambda x: x['date'].year if math.isnan( x['competition_open_since_year'] ) else x['competition_open_since_year'], axis=1 )\n",
        "\n",
        "        #promo2_since_week           \n",
        "        df1['promo2_since_week'] = df1.apply( lambda x: x['date'].week if math.isnan( x['promo2_since_week'] ) else x['promo2_since_week'], axis=1 )\n",
        "\n",
        "        #promo2_since_year           \n",
        "        df1['promo2_since_year'] = df1.apply( lambda x: x['date'].year if math.isnan( x['promo2_since_year'] ) else x['promo2_since_year'], axis=1 )\n",
        "\n",
        "        #promo_interval              \n",
        "        month_map = {1: 'Jan',  2: 'Fev',  3: 'Mar',  4: 'Apr',  5: 'May',  6: 'Jun',  7: 'Jul',  8: 'Aug',  9: 'Sep',  10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
        "\n",
        "        df1['promo_interval'].fillna(0, inplace=True )\n",
        "\n",
        "        df1['month_map'] = df1['date'].dt.month.map( month_map )\n",
        "\n",
        "        df1['is_promo'] = df1[['promo_interval', 'month_map']].apply( lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split( ',' ) else 0, axis=1 )\n",
        "\n",
        "        ## 1.6. Change Data Types\n",
        "        # competiton\n",
        "        df1['competition_open_since_month'] = df1['competition_open_since_month'].astype( int )\n",
        "        df1['competition_open_since_year'] = df1['competition_open_since_year'].astype( int )\n",
        "            \n",
        "        # promo2\n",
        "        df1['promo2_since_week'] = df1['promo2_since_week'].astype( int )\n",
        "        df1['promo2_since_year'] = df1['promo2_since_year'].astype( int )\n",
        "\n",
        "        return df1\n",
        "    \n",
        "    \n",
        "    def feature_engineering( self, df2 ):\n",
        "\n",
        "        # year\n",
        "        df2['year'] = df2['date'].dt.year\n",
        "\n",
        "        # month\n",
        "        df2['month'] = df2['date'].dt.month\n",
        "\n",
        "        # day\n",
        "        df2['day'] = df2['date'].dt.day\n",
        "\n",
        "        # week of year\n",
        "        df2['week_of_year'] = df2['date'].dt.isocalendar().week \n",
        "\n",
        "        # year week\n",
        "        df2['year_week'] = df2['date'].dt.strftime( '%Y-%W' )\n",
        "\n",
        "        # competition since\n",
        "        df2['competition_since'] = df2.apply( lambda x: datetime.datetime( year=x['competition_open_since_year'], month=x['competition_open_since_month'],day=1 ), axis=1 )\n",
        "        df2['competition_time_month'] = ( ( df2['date'] - df2['competition_since'] )/30 ).apply( lambda x: x.days ).astype( int )\n",
        "\n",
        "        # promo since\n",
        "        df2['promo_since'] = df2['promo2_since_year'].astype( str ) + '-' + df2['promo2_since_week'].astype( str )\n",
        "        df2['promo_since'] = df2['promo_since'].apply( lambda x: datetime.datetime.strptime( x + '-1', '%Y-%W-%w' ) - datetime.timedelta( days=7 ) )\n",
        "        df2['promo_time_week'] = ( ( df2['date'] - df2['promo_since'] )/7 ).apply( lambda x: x.days ).astype( int )\n",
        "\n",
        "        # assortment\n",
        "        df2['assortment'] = df2['assortment'].apply( lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended' )\n",
        "\n",
        "        # state holiday\n",
        "        df2['state_holiday'] = df2['state_holiday'].apply( lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day' )\n",
        "        \n",
        "        # 3.0. PASSO 03 - FILTRAGEM DE VARIÁVEIS\n",
        "        ## 3.1. Filtragem das Linhas\n",
        "        df2 = df2[(df2['open'] != 0)]\n",
        "\n",
        "        ## 3.2. Selecao das Colunas\n",
        "        cols_drop = [ 'open', 'promo_interval', 'month_map']\n",
        "        df2 = df2.drop( cols_drop, axis=1 )\n",
        "\n",
        "        return df2\n",
        "\n",
        "    def data_preparation( self, df5, ft ):\n",
        "        \n",
        "        ## 5.2. Rescaling ( Normalizando os ranges  )\n",
        "        # competition distance\n",
        "        df5['competition_distance'] = self.competition_distance_scaler.fit_transform( df5[['competition_distance']].values )\n",
        "\n",
        "\n",
        "        # competition time month\n",
        "        df5['competition_time_month'] = self.competition_time_month_scaler.fit_transform( df5[['competition_time_month']].values )\n",
        "        \n",
        "        # promo time week\n",
        "        df5['promo_time_week'] = self.promo_time_week_scaler.fit_transform( df5[['promo_time_week']].values )\n",
        "\n",
        "        # year\n",
        "        df5['year'] = self.year_scaler.fit_transform( df5[['year']].values )\n",
        "\n",
        "        ### 5.3.1. Enconding\n",
        "        # state_holiday ( One Hot Enconding )\n",
        "        df5 = pd.get_dummies( df5, prefix=['state_holiday'], columns=['state_holiday'] )\n",
        "\n",
        "        # store_type ( Label Encoding )\n",
        "        df5['store_type'] = self.store_type_scaler.fit_transform( df5['store_type'] )\n",
        "\n",
        "\n",
        "        # assortment ( Ordinal Encoding )\n",
        "        assortment_dict = {'basic': 1, 'extra': 2, 'extended': 3}\n",
        "        df5['assortment'] = df5['assortment'].map( assortment_dict )\n",
        "\n",
        "        ### 5.3.3. Nature Transformation ( Natureza Cíclica )\n",
        "        # month\n",
        "        df5['month_sin'] = df5['month'].apply( lambda x: np.sin( x* ( 2. * np.pi/12 ) ) )\n",
        "        df5['month_cos'] = df5['month'].apply( lambda x: np.cos( x* ( 2. * np.pi/12 ) ) )\n",
        "\n",
        "        # day\n",
        "        df5['day_sin'] = df5['day'].apply( lambda x: np.sin( x* ( 2. * np.pi/30 ) ) )\n",
        "        df5['day_cos'] = df5['day'].apply( lambda x: np.cos( x* ( 2. * np.pi/30 ) ) )\n",
        "\n",
        "        # week of year\n",
        "        df5['week_of_year_sin'] = df5['week_of_year'].apply( lambda x: np.sin( x* ( 2. * np.pi/52 ) ) )\n",
        "        df5['week_of_year_cos'] = df5['week_of_year'].apply( lambda x: np.cos( x* ( 2. * np.pi/52 ) ) )\n",
        "\n",
        "        # day of week\n",
        "        df5['day_of_week_sin'] = df5['day_of_week'].apply( lambda x: np.sin( x* ( 2. * np.pi/7 ) ) )\n",
        "        df5['day_of_week_cos'] = df5['day_of_week'].apply( lambda x: np.cos( x* ( 2. * np.pi/7 ) ) )\n",
        "\n",
        "        cols_selected = ['store', 'promo', 'store_type', 'assortment', 'competition_distance', 'competition_open_since_month', 'competition_open_since_year', 'promo2', 'promo2_since_week', \n",
        "                        'promo2_since_year', 'competition_time_month', 'promo_time_week', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'week_of_year_sin', 'week_of_year_cos', \n",
        "                        'day_of_week_sin', 'day_of_week_cos']\n",
        "\n",
        "        return df5[ cols_selected ]\n",
        "\n",
        "    def get_prediction( self, model, original_data, test_data ):\n",
        "\n",
        "        #prediction\n",
        "        pred = model.predict( test_data )\n",
        "\n",
        "        #join pred into the original data\n",
        "        original_data['prediction'] = np.expm1( pred )\n",
        "\n",
        "        return original_data.to_json( orient='records', date_format='iso' )      "
      ],
      "metadata": {
        "id": "10gRcqio0b40"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.2. API Handler "
      ],
      "metadata": {
        "id": "0WJnCTsGTXJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from flask import Flask, request, Response\n",
        "#from rossmann.Rossmann import Rossmann\n",
        "\n",
        "# loading model\n",
        "\n",
        "model = pickle.load( open( '/content/Rossmann_Store_Sales/Model trained/model_rossmann.pkl', 'rb') )\n",
        "\n",
        "# inicialize API\n",
        "app = Flask( __name__) \n",
        "            \n",
        "@app.route( '/rossmann/predict', methods=['POST'] )\n",
        "def rossmann_predict():\n",
        "    test_json = request.get_json()\n",
        "\n",
        "    if test_json: # there is data\n",
        "        if isinstance( test_json, dict ): # unique example\n",
        "            test_raw = pd.DataFrame( test_json, index=[0] )\n",
        "\n",
        "        else: # multiple example\n",
        "            test_raw = pd.DataFrame( test_json, columns=test_json[0].keys() )\n",
        "\n",
        "        # Instantiate Rossmann class\n",
        "        pipeline = Rossmann()\n",
        "\n",
        "        # data cleaning\n",
        "        df1 = pipeline.data_cleaning( test_raw )\n",
        "\n",
        "        # feature enginerering\n",
        "        df2 = pipeline.feature_engineering( df1 )\n",
        "\n",
        "        # data preparation\n",
        "        df3 = pipeline.data_prediction( df2 )\n",
        "\n",
        "        # prediction\n",
        "        df_response = pipeline.get_prediction( model, test_raw, df3 )\n",
        "\n",
        "        return df_response\n",
        "\n",
        "    else:\n",
        "        return Response( '{}', status=200, mimetype='application/json' )\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run( '0.0.0.0' )"
      ],
      "metadata": {
        "id": "ZiXPlpCy0b2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.3. API Tester"
      ],
      "metadata": {
        "id": "_ibg4bQSTc_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "JfrAXKwa5j1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading test dataset\n",
        "df10 = pd.read_csv( '/content/Rossmann_Store_Sales/Dataset/test.csv' )"
      ],
      "metadata": {
        "id": "BMh7oTua0bzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge test dataset + store\n",
        "df_test = pd.merge( df10, df_store_raw, how='left', on='Store' )"
      ],
      "metadata": {
        "id": "Zs7-4M1xLNSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test"
      ],
      "metadata": {
        "id": "kyfdmhubketn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose store for prediction\n",
        "df_test = df_test[df_test['Store'] == 22]\n",
        "\n",
        "#remove closed days\n",
        "#df_test = df_test[df_test['Open_y'] != 0]\n",
        "#df_test = df_test[~df_test['Open'].isnull()]\n",
        "#df_test = df_test.drop( 'Id',axis=1 )\n",
        "df_test.head()"
      ],
      "metadata": {
        "id": "zYWKjw4Gkdss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert Dataframe to json\n",
        "data = json.dumps( df_test.to_dict( orient='records') )"
      ],
      "metadata": {
        "id": "F8StNdjwMz4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API Call\n",
        "url = 'http://0.0.0.0:5000/rossmann/predict' # É o endpoint, onde será enviado\n",
        "header = {'Content-type': 'application/json' }  # Indica o tipo de dado que está recebendo\n",
        "data = data\n",
        "\n",
        "r = requests.post( url, data=data, headers=header )\n",
        "print( 'Status Code {}'.format( r.status_code ) )"
      ],
      "metadata": {
        "id": "emnPTgAtMz1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = pd.DataFrame( r.json(), columns=r.json()[0].keys() )"
      ],
      "metadata": {
        "id": "PYDrntahMzyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2 = d1[['store', 'prediction']].groupby( 'store' ).sum().reset_index()\n",
        "\n",
        "for i in range( len( d2 ) ):\n",
        "    print( 'Store Number {} will sell R${:,.2f} in the next 6 weeks'.format(\n",
        "        d2.loc[i, 'store'],\n",
        "        d2.loc[i, 'prediction'] ) )"
      ],
      "metadata": {
        "id": "LcTS3GLsMzwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kOSmZKBMxni-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Cme0l2fgyTFh",
        "mAFJ7tGZyTFn",
        "XEPkhdtxyTFo",
        "gU_NLcbfyTFo",
        "WFO6TERpyTFp",
        "ZQVDL2bzyTFp",
        "PVc5kcYLyTFq",
        "Q2jyu4OryTFr",
        "sS8hzOB4yTFu",
        "KO2hs5lcyTFu",
        "niSe3joDyTFu",
        "DUeCXTpyyTFv",
        "VRiWMOiSyTFw",
        "4DtHBNRayTFw",
        "cVgxFl_oml9b",
        "m4XkvK0TtN6y",
        "TMdKWQI_tgFv",
        "WIIAH779s09U",
        "MqHndotYAGNi",
        "G3utuDePAGKc",
        "H1Sqpjqknpz-",
        "NHR4BxhwszJZ",
        "B-cSDG6wt02R",
        "4BS4fErVt_mO",
        "XoBuvjNi_t0_",
        "IvKvzhu8CeAC",
        "3bvJiBgiCd9M",
        "agpnBKG-YY0l",
        "_m_ucl1t3ncQ",
        "gJQ4NRCF8oJu",
        "y9cSWGlMdvMZ",
        "f6UAKnOlq_do",
        "7ooXreuhUdQt",
        "yHqRjzSbUnGv",
        "bjw9QbM9UrnN",
        "epUmwLzMtGIg",
        "ixnopyV9uQFg",
        "-J95fe9-7y9S",
        "FKVO7wRBhIQ_",
        "ZOqFh1Bh83Nu",
        "MWQ6OeleiIGk",
        "KH95WOaTU6hu",
        "y99H_AUAip9t",
        "l53P6u6ZDKUm",
        "L2W5uJzrf7rX",
        "yjQAZPoFgEf3",
        "lauhv_lWiWTD",
        "iYbOmkZHilU0",
        "wKoboWOLipaw",
        "lxvl9H17iuMN"
      ],
      "name": "Rossmann_Store_Sales_Modulo_10_Deploy_Model_to_Production.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}